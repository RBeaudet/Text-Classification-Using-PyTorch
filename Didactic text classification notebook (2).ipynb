{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Quora insincere questions (2) </center>\n",
    "\n",
    "Now that we have run our first basic model, we will construct a new model with several small improvements :\n",
    "\n",
    "<i>1)</i> First, we will let the user specify wether they want to use a LSTM or a GRU unit, for the reasons we have explained in the first notebook. Indeed, a LSTM contains much more parameters to learn, which make sthe training process much longer. As GRU and LSTM yield similar results on numerous tasks, it can be convenient to let the user decide for themselves if they want to use a LSTM or a GRU.\n",
    "\n",
    "<i>2)</i> We will also use <b>regularization techniques</b> to prevent overfitting and improve the training process. Batch normalization is used in convolutional neural networks, so we will not make use of it. But we can add <b>dropout</b> layers, i.e. we randomly set some neurons to zero during training in order to prevent the model to learn co-adaptative features. Note that this amounts to train several different models that we aggregate.\n",
    "\n",
    "<i>3)</i> We will let the user decide wether tgey want to use <b>bi-directional recurrent layers</b> or not. Bi-directional RNN consists in reading a sentence in both ways, which should help to better capture the structure of the sentence. The hidden states are then concatenated to form the final hidden state.\n",
    "\n",
    "<i>4)</i> We will also let them decide <b>how many recurrent layers</b> they want to stack. The idea here is that lower layers should capture low-level information whereas higher layers should capture high-level information.\n",
    "\n",
    "<i>5)</i> A parameter `train_embedding` will allow to chose to re-train the embedding matrix or not. Setting this parameter to TRUE will dramatically increase the number of parameters to train, but it should also help to retrain them on the task at hand.\n",
    "\n",
    "---\n",
    "\n",
    "#### The big improvement : self-attention layer\n",
    "\n",
    "In an <b>encoder - decoder</b> network, en attention layer allows the decoder to look at the entire input sequence at every decoding step, so that it can decide what input words are important at any point in time. The basic attention layer works as follow :\n",
    "\n",
    "- We have encoder hidden states $h_1, h_2, ..., h_N$\n",
    "- On timestep $t$, we have the decoder hidden state $s_t$\n",
    "- For this step, we get the attention scores $e_t = (s_t^T h_1, ... s_t^T h_N)$\n",
    "- Take softmax to get attention distribution $\\alpha^t = softmax(e_t)$\n",
    "- Compute the attention output $a_t = \\sum_{i=1}^N \\alpha_i^t h_i$\n",
    "- Concatenate attention output and decoder hidden state $(a_t, s_t)$\n",
    "\n",
    "There are dozens of attention models, which can be grouped into a single framework : <i>given a set of vector values and a vector query, attention is a technique to compute a weighted sum of the values, dependent on the query</i>.\n",
    "\n",
    "<b>Self-Attention</b> is a mechanism that allows to extract different aspects of the sentence into multiple vector representations. Due to its direct access to hidden representations from previous time steps, self-attention relieves some long-term memorization burden from LSTM. Moreover, interpreting the extracted embedding becomes very easy and explicit.\n",
    "\n",
    "We will perform the self attention mechanism proposed by Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou and Yoshua Bengio in their paper <i>\"A Structured Self-Attentive Sentence Embedding\"</i> in 2017.\n",
    "\n",
    "- We have encoder hidden states $H = (h_1, h_2, ..., h_N)^T$ of shape $(n, u)$\n",
    "- The aim is to encode a variable length sentence into a fixed size embedding, which is achieved by choosing a linear combination of the $n$ LSTM hidden vectors in $H$. A vector of weights $a$ is computed : $A = softmax(􏰀w_{s2} \\tanh 􏰀(w_{s1} H^T))$. However, this vector representation usually focuses on a specific component of the sentence, like a special set of related words or phrases. But there can be multiple components in a sentence that together forms the overall semantics of the whole sentence. Thus, to represent the overall semantics of the sentence, we need multiple m’s that focus on different parts of the sentence. We thus compute a matrix weights $A = softmax(􏰀W_{s2} \\tanh 􏰀(W_{s1} H^T))$, where the softmax is performed along the second dimension of its input.\n",
    "- We then compute the sentence embedding $M = AH$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext.vocab import Vectors, GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Select device (GPU or CPU)\n",
    "USE_GPU = False\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create training data\n",
    "random.seed(14)\n",
    "\n",
    "tokenizer = lambda x: x.split()\n",
    "ID = data.Field()\n",
    "TEXT = data.Field(tokenize=tokenizer, init_token='<bos>', eos_token='<eos>', lower=True)\n",
    "TARGET = data.LabelField(dtype=torch.float)\n",
    "train_fields = [('id', None), ('text', TEXT), ('target', TARGET)]\n",
    "\n",
    "# Data\n",
    "train_data = data.TabularDataset(\n",
    "    path='train.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=train_fields\n",
    ")\n",
    "\n",
    "# Split\n",
    "train, val, test = train_data.split(split_ratio=[0.6, 0.2, 0.2], random_state=random.getstate())\n",
    "\n",
    "# Vocab\n",
    "TEXT.build_vocab(train_data, vectors=GloVe(name='6B', dim=300), min_freq=5)\n",
    "TARGET.build_vocab(train_data)\n",
    "\n",
    "batch_size_train = 256\n",
    "batch_size_val = 256\n",
    "batch_size_test = 256\n",
    "\n",
    "# Iterators\n",
    "train_iter = data.BucketIterator(\n",
    "    train,\n",
    "    sort_key=lambda x: len(x.text),  # sort sequences by length (dynamic padding)\n",
    "    batch_size=batch_size_train,  # batch size\n",
    "    device=device  # select device (e.g. CPU)\n",
    ")\n",
    "\n",
    "val_iter = data.BucketIterator(\n",
    "    val,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    batch_size=batch_size_val,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "test_iter = data.Iterator(\n",
    "    test,\n",
    "    batch_size=batch_size_test,\n",
    "    device=device,\n",
    "    train=False,\n",
    "    sort=False,\n",
    "    sort_within_batch=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model architecture\n",
    "\n",
    "In this part, we define the model architecture presented above using as usual the `nn.Module` API. The `self_attention` method is where we perform the self-attention mechanism. The `init_hidden` method is necessary to initialize parameters whose size change depending on the fact that we use a bidirectional RNN or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_matrix,\n",
    "                 hidden_dim,\n",
    "                 da,\n",
    "                 r,\n",
    "                 output_size,\n",
    "                 dropout,\n",
    "                 num_layers=1,\n",
    "                 use_lstm=True, \n",
    "                 bidirectional=True,\n",
    "                 train_embedding=True):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        \"\"\"\n",
    "        A text classification model, made of an embedding matrix, one or several recurrent layers\n",
    "        and a self-attention layer.\n",
    "        \n",
    "        Arguments:\n",
    "        - embedding_matrix: pre-trained embedding matrix of size (vocab_size, embedding_dim).\n",
    "        - hidden_dim: an integer giving the dimension of the hidden state of the recurrent layer.\n",
    "        - da : Number of units in the Attention mechanism.\n",
    "        - r : Number of Attention heads.\n",
    "        - output_size: an integer giving the size of the output (2 for binary classification).\n",
    "        - dropout: a float between 0.0 and 1.0 giving the dropout rate.\n",
    "        - num_layers: (Optional) number of stacked recurrent layers. Default 1.\n",
    "        - use_lstm: (Optional) boolean that indicates wether to use a LSTM layer or not. \n",
    "          When False, use a GRU instead. Default True.\n",
    "        - bidirectional: (Optional) boolean for using a bidirectional recurrent layer. Default True.\n",
    "        - train embedding: (Optional) boolean to know if we fine tune the embedding matrix. \n",
    "          Default True.\n",
    "        \"\"\"\n",
    "        vocab_size = embedding_matrix.shape[0]\n",
    "        embedding_dim = embedding_matrix.shape[1]\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.use_lstm = use_lstm\n",
    "        self.da = da\n",
    "        self.r = r\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        if train_embedding:\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        else:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "\n",
    "        # Recurrent layer\n",
    "        if use_lstm:\n",
    "            self.rnn = nn.LSTM(input_size=embedding_dim, \n",
    "                               hidden_size=hidden_dim, \n",
    "                               num_layers=num_layers, \n",
    "                               bidirectional=bidirectional,\n",
    "                               dropout=dropout,\n",
    "                               batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.GRU(input_size=embedding_dim, \n",
    "                              hidden_size=hidden_dim, \n",
    "                              num_layers=num_layers,\n",
    "                              bidirectional=bidirectional,\n",
    "                              dropout=dropout,\n",
    "                              batch_first=True)\n",
    "            \n",
    "        # Fully connected layer\n",
    "        if bidirectional:\n",
    "            self.fully_connected = nn.Linear(r * hidden_dim * 2, output_size)\n",
    "        else:\n",
    "            self.fully_connected = nn.Linear(r * hidden_dim, output_size)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"  \n",
    "        Perform a forward pass\n",
    "        \n",
    "        Arguments:\n",
    "        - X: tensor of shape (batch_size, sequence_length)\n",
    "        \n",
    "        Returns:\n",
    "        - Output of the linear layer of shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Embeddings layer + dropout\n",
    "        x = self.embedding(x)  # [batch_size, seq_len, embed_dim]\n",
    "        x = self.dropout_layer(x)  # [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        # 2. Recurrent layer(s)\n",
    "        # First, initialize hidden and cell states.\n",
    "        # Note that only the LSTM requires the cell state.\n",
    "        # x is of shape [batch_size, seq_len, hidden_size]\n",
    "        h0, c0 = self._init_hidden(self.num_layers, x.shape[0], self.hidden_dim)\n",
    "        if self.use_lstm:\n",
    "            x, (fn, cn) = self.rnn(x, (h0, c0))\n",
    "        else:\n",
    "            x, fn = self.rnn(x, h0)\n",
    "            \n",
    "        # 3. Attention layer + dropout\n",
    "        x = self.self_attention(x, self.da, self.r)  # [batch_size, r, hidden_dim] \n",
    "        x = self.dropout_layer(x)\n",
    "        \n",
    "        # 4. Final layer\n",
    "        output = self.fully_connected(x.view(x.size()[0], -1))  # [batch_size, 2]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def self_attention(self, x, da, r):\n",
    "        \"\"\"\n",
    "        Attention mechanism in our model. \n",
    "        Attention is used to compute soft alignment scores between each of \n",
    "        the hidden_state and the last hidden_state of the LSTM. \n",
    "\n",
    "        Arguments:\n",
    "        - lstm_output : Output of the LSTM of shape (batch, seq_len, num_directions * hidden_size).\n",
    "          Tensor containing the output features (h_t) from the last layer of the LSTM, for each t.\n",
    "        - da : Number of units in the Attention mechanism.\n",
    "        - r : Number of Attention heads.\n",
    "        \n",
    "        Returns:\n",
    "        - Tensor of size [batch_size, seq_len, r]\n",
    "        \"\"\"\n",
    "        hidden_dim = x.size()[2]\n",
    "        W_s1 = nn.Linear(hidden_dim, da)\n",
    "        W_s2 = nn.Linear(da, r)\n",
    "        \n",
    "        weight_matrix = F.tanh(W_s1(x))  # [batch_size, seq_len, da]\n",
    "        weight_matrix = W_s2(weight_matrix)  # [batch_size, seq_len, r]\n",
    "        weight_matrix = F.softmax(weight_matrix, dim=1)  # [batch_size, seq_len, r]\n",
    "        weight_matrix = weight_matrix.permute(0, 2, 1)  # [batch-size, r, seq_len]\n",
    "        \n",
    "        x = torch.bmm(weight_matrix, x)  # [batch_size, r, hidden_dim]\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def _init_hidden(self, num_layers, batch_size, hidden_dim):\n",
    "        \"\"\"\n",
    "        Initialize hidden states for the recurrent layers\n",
    "        \n",
    "        Arguments:\n",
    "        - num_layers : number of stacked layers (int).\n",
    "        - batch_size : batch size (int).\n",
    "        - hidden_dim : hidden dimension (int).\n",
    "        \n",
    "        Returns:\n",
    "        - A tuple (h0, c0) containing hidden and cell states.\n",
    "        \"\"\"\n",
    "        \n",
    "        # The hidden state is twice as large for bidirectional LSTM\n",
    "        if self.bidirectional:\n",
    "            h0 = torch.zeros(num_layers * 2, batch_size, hidden_dim)\n",
    "            c0 = torch.zeros(num_layers * 2, batch_size, hidden_dim)\n",
    "        else:\n",
    "            h0 = torch.zeros(num_layers, batch_size, hidden_dim)\n",
    "            c0 = torch.zeros(num_layers, batch_size, hidden_dim)\n",
    "            \n",
    "        return (h0, c0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Finally, the training process will be wrapped into a class `Solver` that allows several tasks :\n",
    "- Training a model ;\n",
    "- Compute loss or accuracy history ;\n",
    "- Save a model, in case we want to keep some specific parameters.\n",
    "\n",
    "The core of the `train` method is the same as the one we coded in the previous notebook, but we added additional information to display and an option to save a model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver():\n",
    "    \"\"\"\n",
    "    Ecapsulates all the logic necessary for training text classification\n",
    "    models.\n",
    "    \n",
    "    The Solver accepts both training and validataion data and labels so it can\n",
    "    periodically check classification accuracy on both training and validation\n",
    "    data to watch out for overfitting.\n",
    "    \n",
    "    - To train a model, construct Solver instance, pass the model, dataset, and \n",
    "    various options (learning rate, batch size, etc) to the\n",
    "    constructor. \n",
    "    - Call the train() method to train the model.\n",
    "    - Instance variable solver.loss_history contains a list of all losses \n",
    "    encountered during training and the instance variables \n",
    "    - Instance variables solver.train_acc_history and solver.val_acc_history are lists of the\n",
    "    accuracies of the model on the training and validation set at each epoch.\n",
    "    \n",
    "    Example usage :\n",
    "    model = Model(*args)\n",
    "    solver = Solver(model, \n",
    "                    loader_train,\n",
    "                    loader_val,\n",
    "                    optimizer)\n",
    "    solver.train()\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model,  optimizer, loader_train, loader_val, **kwargs):\n",
    "        \"\"\"\n",
    "        Required arguments:\n",
    "        - model: A model constructed from PyTorch nn.Module.\n",
    "        - optimizer: An Optimizer object we will use to train the model.\n",
    "        - loader_train: An Iterator object on which iterating to construct batches of \n",
    "          training data.\n",
    "        - loader_val: An Iterator object on which iterating to construct batches of \n",
    "          validation data.\n",
    "          \n",
    "        Optional arguments:\n",
    "        - verbose: Boolean; if set to false then no output will be printed\n",
    "          during training.\n",
    "        - save_model: Boolean; if set to True then save best model.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loader_train = loader_train\n",
    "        self.loader_val = loader_val\n",
    "\n",
    "        # Unpack arguments\n",
    "        self.verbose = kwargs.pop('verbose', False)\n",
    "        self.save_model = kwargs.pop('save_model', False)\n",
    "        \n",
    "        self._reset()\n",
    "        \n",
    "        \n",
    "    def _reset(self):\n",
    "        \"\"\"\n",
    "        Reset some variables for book-keeping:\n",
    "        - best validation accuracy\n",
    "        - loss history\n",
    "        - train accuracy history\n",
    "        - validation accuracy history\n",
    "        - best model parameters\n",
    "        \"\"\"\n",
    "        self.best_val_accuracy = 0\n",
    "        self.loss_history = []\n",
    "        self.train_accuracy_history = []\n",
    "        self.val_accuracy_history = []\n",
    "        self.best_params = {}\n",
    "        \n",
    "        \n",
    "    def _select_device(self, verbose=True):\n",
    "        \"\"\"\n",
    "        Select device e.g. CPU / GPU\n",
    "        \"\"\"\n",
    "        use_gpu = False\n",
    "        if use_gpu and torch.cuda.is_available():\n",
    "            device = torch.device('cuda')\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "        if verbose:\n",
    "            print('Using device:', device)\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "    \n",
    "    def train(self, print_every=10, epochs=1):\n",
    "        \"\"\"\n",
    "        Train a model using the PyTorch Module API.\n",
    "\n",
    "        Arguments:\n",
    "        - print_every: (Optional) Print training accuracy every print_every iterations.\n",
    "        - epochs: (Optional) A Python integer giving the number of epochs to train for.\n",
    "\n",
    "        Returns: Nothing, but prints model accuracies during training.\n",
    "        \"\"\"\n",
    "\n",
    "        # Move the model parameters to CPU / GPU\n",
    "        model = self.model.to(device=self.device)\n",
    "        optimizer = self.optimizer\n",
    "        \n",
    "        # Initialize iteration\n",
    "        t = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "            for train_batch in self.loader_train:\n",
    "\n",
    "                # Put model to training mode\n",
    "                model.train()\n",
    "\n",
    "                # Load x and y\n",
    "                x = train_batch.text.transpose(1, 0)  # reshape to [batch_size, len_seq]\n",
    "                y = train_batch.target.type(torch.LongTensor)\n",
    "\n",
    "                # Move to device, e.g. CPU\n",
    "                x = x.to(device=self.device)\n",
    "                y = y.to(device=self.device)\n",
    "\n",
    "                # Compute scores and softmax loss\n",
    "                scores = model(x)\n",
    "                loss = F.cross_entropy(scores, y)\n",
    "\n",
    "                # Zero out all of the gradients for the variables which the optimizer\n",
    "                # will update.\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Backwards pass: compute the gradient of the loss with\n",
    "                # respect to each parameter of the model.\n",
    "                loss.backward()\n",
    "\n",
    "                # Update the parameters of the model using the gradients\n",
    "                # computed by the backwards pass.\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Save loss\n",
    "                self.loss_history.append(loss.item())\n",
    "\n",
    "                # Display information\n",
    "                if self.verbose and t % print_every == 0:\n",
    "                    print('Iteration %d, loss = %.4f' % (t, self.loss_history[-1]))\n",
    "                    acc = self.compute_accuracy(validation=True)\n",
    "                    print('Accuracy :', acc)\n",
    "                    print()\n",
    "                \n",
    "                t += 1\n",
    "                \n",
    "            end = time.time()\n",
    "            print('Epoch {0} / {1}, time = {2} secs'.format(epoch, epochs, end-start))\n",
    "            \n",
    "            # Compute train and val accuracy at the end of each epoch.\n",
    "            train_accuracy = self.compute_accuracy(validation=False)\n",
    "            val_accuracy = self.compute_accuracy(validation=True)\n",
    "            \n",
    "            self.train_accuracy_history.append(train_accuracy)\n",
    "            self.val_accuracy_history.append(val_accuracy)\n",
    "            \n",
    "            # Print useful information\n",
    "            if self.verbose:\n",
    "                print('(Epoch %d / %d) Train acc: %f; Val acc: %f' % (epoch, epochs, \n",
    "                                                                      train_accuracy, val_accuracy))\n",
    "\n",
    "            # Keep track of the best model\n",
    "            if val_accuracy > self.best_val_accuracy:\n",
    "                self.best_val_accuracy = val_accuracy\n",
    "                # update best params\n",
    "                self.best_params['state_dict'] = model.state_dict().copy()\n",
    "                self.best_params['optimizer'] = optimizer.state_dict().copy()\n",
    "                    \n",
    "        # Save best model\n",
    "        if self.save_model:\n",
    "            self._save_model('/Users/robin/Projects/zelros/', \n",
    "                             self.best_params['state_dict'], \n",
    "                             self.best_params['optimizer'])\n",
    "                \n",
    "                \n",
    "    def compute_accuracy(self, validation=True):\n",
    "        \"\"\"\n",
    "        Compute accuracy of a model.\n",
    "        \n",
    "        Arguments:\n",
    "        - validation: (Optional) If True, compute accuracy on the validation dataset.\n",
    "        \"\"\"\n",
    "        if validation:\n",
    "            loader = self.loader_val\n",
    "        else:\n",
    "            loader = self.loader_train\n",
    "                        \n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "\n",
    "        # Set model to evaluation mode : This has any effect only on certain modules. \n",
    "        # For example, behaviors of dropout layers during train or test differ.\n",
    "        self.model.eval()\n",
    "\n",
    "        # Tell PyTorch not to build computational graphs\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "\n",
    "                # Load x and y\n",
    "                x = batch.text.transpose(1, 0)  # reshape to [batch_size, len_seq]\n",
    "                y = batch.target.type(torch.LongTensor)\n",
    "\n",
    "                # Move to device, e.g. CPU\n",
    "                x = x.to(device=self.device)  \n",
    "                y = y.to(device=self.device)\n",
    "\n",
    "                # Compute scores and predictions\n",
    "                scores = self.model(x)\n",
    "                _, preds = scores.max(1)\n",
    "                num_correct += (preds == y).sum()\n",
    "                num_samples += preds.size(0)\n",
    "\n",
    "        acc = float(num_correct) / num_samples\n",
    "        return acc\n",
    "    \n",
    "    \n",
    "    def _save_model(self, model_path, model_dict, optimizer_dict):\n",
    "        \"\"\"\n",
    "        Save model parameters if we have to interrupt the training.\n",
    "        Parameters are saved in a dictionary.\n",
    "        \n",
    "        Required arguments:\n",
    "        - model_path: where to save the model.\n",
    "        - model_dict: Python dictionary that maps each layer to its parameter tensor.\n",
    "        - optimizer_dict: Python dictionary that contains info about the optimizer's \n",
    "          states and hyperparameters used.\n",
    "        \"\"\"\n",
    "    \n",
    "        state = {\n",
    "            'state_dict': model_dict,   # model.state_dict()\n",
    "            'optimizer' : optimizer_dict   # optimizer.state_dict()\n",
    "        }\n",
    "        filename = model_path + 'best_model.pkl'\n",
    "        torch.save(state, filename)\n",
    "        print('Model saved to %s' % filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "Th way to put all the things together is as follow :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# Model\n",
    "model = TextClassificationModel(embedding_matrix=TEXT.vocab.vectors, \n",
    "                                hidden_dim=64,\n",
    "                                da=100,\n",
    "                                r=5,\n",
    "                                output_size=2,\n",
    "                                dropout=0.5,\n",
    "                                num_layers=1,\n",
    "                                use_lstm=True,\n",
    "                                bidirectional=True,\n",
    "                                train_embedding=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=learning_rate, \n",
    "                       betas=(0.9, 0.999),  # recommended values\n",
    "                       eps=1e-08)  # recommended value\n",
    "\n",
    "# Solver\n",
    "solver = Solver(model=model, \n",
    "                optimizer=optimizer, \n",
    "                loader_train=train_iter, \n",
    "                loader_val=val_iter, \n",
    "                verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solver.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zelros_env",
   "language": "python",
   "name": "zelros_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
