{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Quora insincere questions </center>\n",
    "\n",
    "##  1. Prepare and load data \n",
    "\n",
    "In this part, we will do the preprocessing to handle text data. Recall that the task at hand here is to automatically classify Quora questions to detect sincere from insincere questions.\n",
    "\n",
    "The main steps to prepare text data are the following :\n",
    "- Read the data ;\n",
    "- Do a bit of preprocessing, i.e. remove special characters, numbers, etc. This step really depends on the task ;\n",
    "- Tokenize the text, i.e. each sentence is transformed into a list of words ;\n",
    "- Build the vocabulary, i.e. map each word to a unique integer, handling beginning and ending of sentence, as well as unknown words ;\n",
    "- Convert the tokenized text to the corresponding list of integers built by the mapping ;\n",
    "- Pas the sentences in order to create batches of same length.\n",
    "\n",
    "These steps can be a bit long. We will use ```torchtext```, a library specifically designed to handle text data and load them in the appropriate format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load train and test datasets\n",
    "train_csv = pd.read_csv(\"train.csv\")\n",
    "test_csv = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1306122, 3)\n",
      "(375806, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_csv.shape)\n",
    "print(test_csv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qid              False\n",
       "question_text    False\n",
       "target           False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing values\n",
    "train_csv.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing value. We see here that we have 3 fields : `qid` corresponds to the question unique ID, `question_text` corresponds to the question itself and `target` corresponds to the label, i.e. 0 if sincere question, 1 if insincere question. The test dataset does not contain this `target` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sincere questions 1225312\n",
      "Number of insincere questions 80810\n"
     ]
    }
   ],
   "source": [
    "print('Number of sincere questions', train_csv[train_csv['target'] == 0].shape[0])\n",
    "print('Number of insincere questions', train_csv[train_csv['target'] == 1].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is unbalanced. We have 80,810 examples for positive targets though, which is clearly enough for a classifier to learn. Thus, we will not perform any <b>subsampling</b> or <b>oversampling</b> technique.\n",
    "\n",
    "<i> The dataset is <b>huge</b>. As I do not possess any GPU, I will probably make small splits in order to be able to train the model.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torchtext\n",
    "\n",
    "Torchtext takes in our csv files and converts them to ```Datasets``` objects. A, ```Iterator``` object tgen iterates over this ```Dataset``` object to construct batches of data, handling various steps described above (converting words to integers, constructing batches, etc.).\n",
    "\n",
    "First, we will define ```Fields``` to load the data. In our case, the data have 3 headers : ```qid```, ```question_text``` and ```target```. Declaring fields simply consists in telling torchtext about this structure and telling it that the ```question_text``` field should be treated as text, whereas the ```target``` field should be treated as label. \n",
    "\n",
    "We will define a custom tokenizer to handle the tokenization. The argument `lower=True` will tell torchtext to convert the text to lowercase. We could use a tokenizer from `nltk`, but we will create a simple one. The `sequential` argument is set to true when we are dealing with text data. For the TARGET field in the cell below, we will thus use `sequential=False` and `use_vocab=False`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "\n",
    "# Define a custom tokenizer\n",
    "# We could also use `from nltk import word_tokeniz` for a more efficient tokenizer\n",
    "tokenizer = lambda x: x.split()\n",
    "\n",
    "ID = data.Field()\n",
    "TEXT = data.Field(tokenize=tokenizer, init_token='<bos>', eos_token='<eos>', lower=True)\n",
    "TARGET = data.LabelField(dtype=torch.float)\n",
    "\n",
    "# Declare field for training set\n",
    "train_fields = [('id', None), ('text', TEXT), ('target', TARGET)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't be using the test dataset because we do not have the targets to measure the performances. Instead, we will split our training set into a training dataset and a validation dataset.\n",
    "\n",
    "The `TabularDataset` class allows to construct a `Dataset` object for data whose format is typically a csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can split the data into a <b>training</b>, a <b>validation</b> and a <b>test</b> set. if we want, in order to test our implementation, we can also define a <b>development</b> set as a small subset of the training set. As such, computation time would be much faster and it would help us to debug the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(14)  # for reproductibility\n",
    "\n",
    "# Create our train data\n",
    "train_data = data.TabularDataset(\n",
    "    path='train.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=train_fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train, validation and test datasets\n",
    "train, val, test = train_data.split(split_ratio=[0.6, 0.2, 0.2], random_state=random.getstate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look a what `train` looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Dataset.__getattr__ at 0x18ff2d0c0>\n",
      "<torchtext.data.example.Example object at 0x14858b6a0>\n",
      "dict_keys(['text', 'target'])\n"
     ]
    }
   ],
   "source": [
    "print(train.type)\n",
    "print(train[0])\n",
    "print(train[0].__dict__.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['is',\n",
       "  'it',\n",
       "  'safe',\n",
       "  'to',\n",
       "  'use',\n",
       "  'american',\n",
       "  'appliances',\n",
       "  '(110v)',\n",
       "  'in',\n",
       "  'india?'],\n",
       " 'target': '0'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One training example\n",
    "vars(train.examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 783673\n",
      "Number of validation examples: 261225\n",
      "Number of test examples: 261224\n"
     ]
    }
   ],
   "source": [
    "print('Number of training examples:', len(train))\n",
    "print('Number of validation examples:', len(val))\n",
    "print('Number of test examples:', len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.dataset.Dataset'>\n",
      "<class 'torchtext.data.dataset.Dataset'>\n",
      "<class 'torchtext.data.dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train))\n",
    "print(type(val))\n",
    "print(type(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build the vocabulary for the `TEXT` field from from the entire training dataset. We can load pre-trained word vectors to embed the tokens. We will use <b>Glove</b> vectors, a common choice for word embedding. Moreover, the construction of Glove vectors ensure that the embedding takes into account both global statistics (like counts or frequencies in a corpus) and semantic information (like Word2Vec would do)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was somehow unable to load pre-trained vectors from the zip file provided by Kaggle using this command :\n",
    "`TEXT.vocab.load_vectors(torchtext.vocab.Vectors('glove.840B.300d/glove.840B.300d.txt'))`\n",
    "\n",
    "Hence, I used the available Glove vectors from torchtext directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import Vectors, GloVe\n",
    "\n",
    "# minimum frequency needed to include a token in the vocabulary set to 5\n",
    "TEXT.build_vocab(train_data, vectors=GloVe(name='6B', dim=300), min_freq=5)\n",
    "TARGET.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocab: 80258\n",
      "Unique tokens in TARGET vocab: 2\n"
     ]
    }
   ],
   "source": [
    "print('Unique tokens in TEXT vocab:', len(TEXT.vocab))\n",
    "print('Unique tokens in TARGET vocab:', len(TARGET.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a look a our embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text vocabulary: 80258\n",
      "Embedding size of text vocabulary:  torch.Size([80258, 300])\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = TEXT.vocab.vectors\n",
    "print (\"Length of text vocabulary: \" + str(len(TEXT.vocab)))\n",
    "print (\"Embedding size of text vocabulary: \", TEXT.vocab.vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have a vocabulary of length 117,676 and an embedding size of 300. The first words in our vocabulary are special tokens `<unk>`, `<pad>`, `<bos>` and `<eos>`. We can access this mapping with the arguments `itos` and `stoi`. As we can see, they are all initialized to 0 vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "<pad>\n",
      "<bos>\n",
      "<eos>\n",
      "the\n",
      "what\n",
      "is\n",
      "a\n",
      "to\n",
      "in\n"
     ]
    }
   ],
   "source": [
    "# Print first tokens in vocabulary\n",
    "for i in range(10):\n",
    "    print(TEXT.vocab.itos[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print embeddings for `<bos>` token\n",
    "word_embeddings[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.6560e-02,  2.1318e-01, -7.4364e-03, -4.5854e-01, -3.5639e-02,\n",
       "         2.3643e-01, -2.8836e-01,  2.1521e-01, -1.3486e-01, -1.6413e+00,\n",
       "        -2.6091e-01,  3.2434e-02,  5.6621e-02, -4.3296e-02, -2.1672e-02,\n",
       "         2.2476e-01, -7.5129e-02, -6.7018e-02, -1.4247e-01,  3.8825e-02,\n",
       "        -1.8951e-01,  2.9977e-01,  3.9305e-01,  1.7887e-01, -1.7343e-01,\n",
       "        -2.1178e-01,  2.3617e-01, -6.3681e-02, -4.2318e-01, -1.1661e-01,\n",
       "         9.3754e-02,  1.7296e-01, -3.3073e-01,  4.9112e-01, -6.8995e-01,\n",
       "        -9.2462e-02,  2.4742e-01, -1.7991e-01,  9.7908e-02,  8.3118e-02,\n",
       "         1.5299e-01, -2.7276e-01, -3.8934e-02,  5.4453e-01,  5.3737e-01,\n",
       "         2.9105e-01, -7.3514e-03,  4.7880e-02, -4.0760e-01, -2.6759e-02,\n",
       "         1.7919e-01,  1.0977e-02, -1.0963e-01, -2.6395e-01,  7.3990e-02,\n",
       "         2.6236e-01, -1.5080e-01,  3.4623e-01,  2.5758e-01,  1.1971e-01,\n",
       "        -3.7135e-02, -7.1593e-02,  4.3898e-01, -4.0764e-02,  1.6425e-02,\n",
       "        -4.4640e-01,  1.7197e-01,  4.6246e-02,  5.8639e-02,  4.1499e-02,\n",
       "         5.3948e-01,  5.2495e-01,  1.1361e-01, -4.8315e-02, -3.6385e-01,\n",
       "         1.8704e-01,  9.2761e-02, -1.1129e-01, -4.2085e-01,  1.3992e-01,\n",
       "        -3.9338e-01, -6.7945e-02,  1.2188e-01,  1.6707e-01,  7.5169e-02,\n",
       "        -1.5529e-02, -1.9499e-01,  1.9638e-01,  5.3194e-02,  2.5170e-01,\n",
       "        -3.4845e-01, -1.0638e-01, -3.4692e-01, -1.9024e-01, -2.0040e-01,\n",
       "         1.2154e-01, -2.9208e-01,  2.3353e-02, -1.1618e-01, -3.5768e-01,\n",
       "         6.2304e-02,  3.5884e-01,  2.9060e-02,  7.3005e-03,  4.9482e-03,\n",
       "        -1.5048e-01, -1.2313e-01,  1.9337e-01,  1.2173e-01,  4.4503e-01,\n",
       "         2.5147e-01,  1.0781e-01, -1.7716e-01,  3.8691e-02,  8.1530e-02,\n",
       "         1.4667e-01,  6.3666e-02,  6.1332e-02, -7.5569e-02, -3.7724e-01,\n",
       "         1.5850e-02, -3.0342e-01,  2.8374e-01, -4.2013e-02, -4.0715e-02,\n",
       "        -1.5269e-01,  7.4980e-02,  1.5577e-01,  1.0433e-01,  3.1393e-01,\n",
       "         1.9309e-01,  1.9429e-01,  1.5185e-01, -1.0192e-01, -1.8785e-02,\n",
       "         2.0791e-01,  1.3366e-01,  1.9038e-01, -2.5558e-01,  3.0400e-01,\n",
       "        -1.8960e-02,  2.0147e-01, -4.2110e-01, -7.5156e-03, -2.7977e-01,\n",
       "        -1.9314e-01,  4.6204e-02,  1.9971e-01, -3.0207e-01,  2.5735e-01,\n",
       "         6.8107e-01, -1.9409e-01,  2.3984e-01,  2.2493e-01,  6.5224e-01,\n",
       "        -1.3561e-01, -1.7383e-01, -4.8209e-02, -1.1860e-01,  2.1588e-03,\n",
       "        -1.9525e-02,  1.1948e-01,  1.9346e-01, -4.0820e-01, -8.2966e-02,\n",
       "         1.6626e-01, -1.0601e-01,  3.5861e-01,  1.6922e-01,  7.2590e-02,\n",
       "        -2.4803e-01, -1.0024e-01, -5.2491e-01, -1.7745e-01, -3.6647e-01,\n",
       "         2.6180e-01, -1.2077e-02,  8.3190e-02, -2.1528e-01,  4.1045e-01,\n",
       "         2.9136e-01,  3.0869e-01,  7.8864e-02,  3.2207e-01, -4.1023e-02,\n",
       "        -1.0970e-01, -9.2041e-02, -1.2339e-01, -1.6416e-01,  3.5382e-01,\n",
       "        -8.2774e-02,  3.3171e-01, -2.4738e-01, -4.8928e-02,  1.5746e-01,\n",
       "         1.8988e-01, -2.6642e-02,  6.3315e-02, -1.0673e-02,  3.4089e-01,\n",
       "         1.4106e+00,  1.3417e-01,  2.8191e-01, -2.5940e-01,  5.5267e-02,\n",
       "        -5.2425e-02, -2.5789e-01,  1.9127e-02, -2.2084e-02,  3.2113e-01,\n",
       "         6.8818e-02,  5.1207e-01,  1.6478e-01, -2.0194e-01,  2.9232e-01,\n",
       "         9.8575e-02,  1.3145e-02, -1.0652e-01,  1.3510e-01, -4.5332e-02,\n",
       "         2.0697e-01, -4.8425e-01, -4.4706e-01,  3.3305e-03,  2.9264e-03,\n",
       "        -1.0975e-01, -2.3325e-01,  2.2442e-01, -1.0503e-01,  1.2339e-01,\n",
       "         1.0978e-01,  4.8994e-02, -2.5157e-01,  4.0319e-01,  3.5318e-01,\n",
       "         1.8651e-01, -2.3622e-02, -1.2734e-01,  1.1475e-01,  2.7359e-01,\n",
       "        -2.1866e-01,  1.5794e-02,  8.1754e-01, -2.3792e-02, -8.5469e-01,\n",
       "        -1.6203e-01,  1.8076e-01,  2.8014e-02, -1.4340e-01,  1.3139e-03,\n",
       "        -9.1735e-02, -8.9704e-02,  1.1105e-01, -1.6703e-01,  6.8377e-02,\n",
       "        -8.7388e-02, -3.9789e-02,  1.4184e-02,  2.1187e-01,  2.8579e-01,\n",
       "        -2.8797e-01, -5.8996e-02, -3.2436e-02, -4.7009e-03, -1.7052e-01,\n",
       "        -3.4741e-02, -1.1489e-01,  7.5093e-02,  9.9526e-02,  4.8183e-02,\n",
       "        -7.3775e-02, -4.1817e-01,  4.1268e-03,  4.4414e-01, -1.6062e-01,\n",
       "         1.4294e-01, -2.2628e+00, -2.7347e-02,  8.1311e-01,  7.7417e-01,\n",
       "        -2.5639e-01, -1.1576e-01, -1.1982e-01, -2.1363e-01,  2.8429e-02,\n",
       "         2.7261e-01,  3.1026e-02,  9.6782e-02,  6.7769e-03,  1.4082e-01,\n",
       "        -1.3064e-02, -2.9686e-01, -7.9913e-02,  1.9500e-01,  3.1549e-02,\n",
       "         2.8506e-01, -8.7461e-02,  9.0611e-03, -2.0989e-01,  5.3913e-02])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print embeddings for `the` token\n",
    "word_embeddings[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use an Iterator to build batches\n",
    "\n",
    "Now, an `Iterator` will iterate over the datasets to construct the batches. The nice thing with Torchtext is that it can handle <b>dynamic padding</b> itself. Basically, sentences are sequences of tokens of different sizes, but batches need to be of fixes size, for example (256, 32), where 32 is the length of the sequences in the batch.\n",
    "\n",
    "In order to have sentences of same length, we <b>zero-pad</b> them within a batch so that they all have the length of the longest sequence in the batch. To be more efficient, we create batches of sentences with approximately the same length so that we avoid too much zero-padding. This is called dynamic padding, and luckily Torchtext will take care of it for us, i.e. it will shuffle the data and create batches made of similar length sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Select device (GPU or CPU)\n",
    "\n",
    "USE_GPU = False\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 256\n",
    "batch_size_val = 256\n",
    "batch_size_test = 256\n",
    "\n",
    "# Iterator for training set\n",
    "train_iter = data.BucketIterator(\n",
    "    train,\n",
    "    sort_key=lambda x: len(x.text),  # sort sequences by length (dynamic padding)\n",
    "    batch_size=batch_size_train,  # batch size\n",
    "    device=device  # select device (e.g. CPU)\n",
    ")\n",
    "\n",
    "# Iterator for validation set\n",
    "val_iter = data.BucketIterator(\n",
    "    val,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    batch_size=batch_size_val,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Don't want to shuffle test data, so we use a standard iterator\n",
    "test_iter = data.Iterator(\n",
    "    test,\n",
    "    batch_size=batch_size_test,\n",
    "    device=device,\n",
    "    train=False,\n",
    "    sort=False,\n",
    "    sort_within_batch=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap in a function\n",
    "\n",
    "Now that we have all the elements, we can wrap them into a single function `load_data` if we want to perform all of the above steps at once.\n",
    "\n",
    "The function could for instance return `train_iter`, `val_iter`, `test_iter`, `dev_iter` as well as the embedding matrix `word_embeddings` and the vocabulary `TEXT.vocab`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model architecture\n",
    "\n",
    "We will implement a simple model in Pytorch that is designed to handle text data. We will use a recurrent neural network with the following architecture :\n",
    "- <b>Embedding layer</b>\n",
    "- <b>GRU</b> or <b>LSTM</b> layer\n",
    "- <b>Fully connected layer</b>\n",
    "\n",
    "The GRU unit is computationally more efficient than a LSTM unit (less parameters to update), and in most cases results are quite similar. The default choice would be to use a LSTM unit, but for this example we can use a GRU.\n",
    "\n",
    "We will implement this model using the Pytorch `nn.Module` API which requires to declare the different layers in the `init` and the forward pass in a `forward` function. The `nn.Module` API allows to define arbitrary network architectures, while tracking every learnable parameters. PyTorch also provides the `torch.optim` package that implements all the common optimizers, such as RMSProp, Adagrad, and Adam.\n",
    "\n",
    "#### Why using a recurrent unit ?\n",
    "\n",
    "A recurrent unit will proceed text sequentially using the same parameters, which is suitable to encode a complete sentence. It contains a <b>hidden state</b> and several gates that allow to select the information it needs at each time step to update this hidden state. Recurrent models have become the <i>default choice for handling NLP tasks</i> and tend to perform better than other architectures such as Convolutional networks (more suitable to image data). Another advantage of recurrent layers is that it can process sequences of arbitrary length.\n",
    "\n",
    "#### LSTM in details\n",
    "\n",
    "On step t, there is a hidden state `ht`and a celle state `ct`. The cell stores long term information, and the LSTM can erase, write and read information from the cell. The information that is erased, written and red is controlled by gates, whose value is dynamic (i.e. computed based on the current context) :\n",
    "- forget gate : $f_t = \\sigma (W_f h_{t-1} + U_f x_t + b_f)$ (which information are we going to throw away)\n",
    "- input gate : $i_t = \\sigma (W_i h_{t-1} + U_i x_t + b_i)$ (which values are we going to update)\n",
    "- vector of candidates : $\\hat{c_t} = \\tanh (W_c h_{t-1} + U_c x_t + b_c)$ (create a vector of candidates for updating)\n",
    "- cell state : $c_t = f_t  c_{t_1} + i_t  \\hat{c_t}$ (erases some content from last cell and writes some new content)\n",
    "- output gate : $o_t = \\sigma (W_o h_{t-1} + U_o x_t + b_o)$ (what part of the cell state are we going to output)\n",
    "- hidden state : $h_t = o_t  tanh(c_t)$\n",
    "\n",
    "A GRU does not have a celle state. Rather, an update gate controls what parts of the hidden state are updated and a reset gate controls what parts of previous hidden state are used to compute new content.\n",
    "\n",
    "We thus see that a LSTM / GRU is a good model to encode a sentence in a final hidden state $h_T$.\n",
    "\n",
    "##### Potential Improvements\n",
    "\n",
    "- <b>Stacked RNN</b> : the lower RNN should compute low level features, and the higher RNN should compute high level features ;\n",
    "- <b>Bi-directional RNN</b> : reads the sentence from left to right and right to left, which allows to better encode the sentence ;\n",
    "- <b>Deep bi-directional RNN</b> : combination of the two previous improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_size):\n",
    "        super(NaiveModel, self).__init__()\n",
    "        \"\"\"\n",
    "        A simple text classification model based on the following architecture :\n",
    "        Embeddings -> LSTM / GRU -> Linear\n",
    "        \n",
    "        Arguments:\n",
    "        - batch_size: batch size\n",
    "        - embedding_matrix: pre-trained embedding matrix of size (vocab_size, embedding_dim).\n",
    "        - hidden_dim: an integer giving the dimension of the hidden state of the recurrent layer.\n",
    "        - output_size: size of output (2 for binary classification)\n",
    "        \"\"\"\n",
    "                \n",
    "        vocab_size = embedding_matrix.shape[0]\n",
    "        embedding_dim = embedding_matrix.shape[1]\n",
    "\n",
    "        # 1. Embedding layer\n",
    "        # We initialize the embedding matrix using our pre-trained Glove vectors.\n",
    "        # The `require_grad` argument tells PyTorch that this matrix should not be\n",
    "        # updated during training.\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        # 2. Recurrent layer\n",
    "        # The `batch_first` argument tells PyTorch that the data will contain batch size\n",
    "        # as first dimension\n",
    "        self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "            \n",
    "        # 3. Fully connected layer\n",
    "        self.fully_connected = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"  \n",
    "        Perform a forward pass\n",
    "        \n",
    "        Arguments:\n",
    "        - X: tensor of shape (batch_size, sequence_length)\n",
    "        \n",
    "        Returns:\n",
    "        - Output of the linear layer of shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Embeddings layer\n",
    "        x = self.embedding(x)  # [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        # 2. Recurrent layer \n",
    "        # By default, the hidden state and cell state are initialized to zero\n",
    "        x, (hn, cn) = self.rnn(x)  # hn is of shape [1, batch_size, hidden_dim]\n",
    "            \n",
    "        # 3. Final layer\n",
    "        # The `squeeze()` function allows to put hn in shape [batch_size, hidden_dim]\n",
    "        output = self.fully_connected(torch.squeeze(hn))  # [batch_size, 2]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Once our model is defined, we can train it using a simple `for` loop. In Pytorch, we must first declare  an optimizer and tell that we are in training mode. We can then iterate over batches of data and update the parameters using gradient descent. \n",
    "\n",
    "Several optimization algorithms are available :\n",
    "<b>- Stochastic gradient descent (SGD)\n",
    "- SGD with momentum\n",
    "- Nesterov momentum\n",
    "- Adagrad\n",
    "- RMSProp\n",
    "- Adam</b>\n",
    "\n",
    "Basically, simple SGD is not really efficient because gradients can be high in some directions and low in others, provoking an undesirable zigzagging movement. `SGD with momentum` tries to tackle this issue by computing a momentum, which is kind of a historic of gradients in order to better orient gradient descent (this is analoguous to velocity in physics). \n",
    "\n",
    "`Adagrad` allows to directly control the learning rate by computing a sum of squares of gradients. Thus, if gradients are high in one direction, we will divide the learning rate by a large value, making the updates smaller. On the contrary, weights that receive small updates will have their learning rate increasedAdagrad allows to directly control the learning rate by computing a sum of squares of gradients. Thus, if gradients are high in one direction, we will divide the learning rate by a large value, making the updates smaller. On the contrary, weights that receive small updates will have their learning rate increased.\n",
    "\n",
    "`RMSProp` is similar to `Adagrad` but attempts to reduce its aggressive, monotonically decreasing learning rate, by using a moving average of squared gradients instead.\n",
    "\n",
    "Finally, `Adam` is a combination of `RMSProp` and momentum techniques, and is the recommended default choice in most problems. This is the one we will use. The update rule is :\n",
    "- $m = \\beta m + (1-\\beta)dx$\n",
    "- $m_t = m / (1-\\beta)^2$\n",
    "- $v = \\beta_2 v + (1-\\beta_2)dx^2$\n",
    "- $v_t = v / (1 - \\beta_2)^2$\n",
    "- $x += - lr \\times m_t / (\\sqrt{v_t} + \\epsilon)$\n",
    "\n",
    "We use $\\beta=0.9$ and $\\beta_2 = 0.999$.\n",
    "\n",
    "We also have to define a loss function for our problem. We will use the <b>cross entropy loss</b>, a common choice for classification tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define a function to test the accuracy of a model either on the training or on the validation dataset. \n",
    "\n",
    "Several metrics are available for classification tasks, such as :\n",
    "<b>- accuracy\n",
    "- recall\n",
    "- precision\n",
    "- F1 score\n",
    "- ROC AUC</b>\n",
    "\n",
    "Choosing the right metric is essential but is really problem-dependent (e.g. do we want to classify all positive targets accurately, even if this implies misclassifying a lot of negative targets ?). For simplicity, we will use accuracy as our main metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_naive(loader, model, validation=True):\n",
    "    \"\"\"\n",
    "    Check accuracy of a model.\n",
    "    \n",
    "    Arguments:\n",
    "    - model: A PyTorch Module giving the model.\n",
    "    - loader_train: An Iterator object on which iterating to construct batches of data.\n",
    "    - validation: (Optionnal) boolean which indicates if we check accuracy on the training\n",
    "      or validation dataset.\n",
    "    \n",
    "    Returns:\n",
    "    - prints the accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use validation or test set\n",
    "    if validation:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')\n",
    "        \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "    # Set model to evaluation mode : This has any effect only on certain modules. \n",
    "    # For example, behaviors of dropout layers during train or test differ.\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():  # Indicate to PyTorch that we don't need to build computational graphs\n",
    "        for t, batch in enumerate(loader):\n",
    "            \n",
    "            # Load x and y\n",
    "            x = batch.text.transpose(1, 0)  # reshape to [batch_size, len_seq]\n",
    "            y = batch.target.type(torch.LongTensor)\n",
    "            \n",
    "            # Move to device, e.g. CPU\n",
    "            x = x.to(device=device)  \n",
    "            y = y.to(device=device)\n",
    "            \n",
    "            # Compute scores and predictions\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "            \n",
    "            if t == 10:\n",
    "                break\n",
    "            \n",
    "        acc = float(num_correct) / num_samples\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive(model, optimizer, loader_train, print_every=10, epochs=1, stop=True):\n",
    "    \"\"\"\n",
    "    Train a model using the PyTorch Module API.\n",
    "    \n",
    "    Arguments:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model.\n",
    "    - loader_train: An Iterator object on which iterating to construct batches of data.\n",
    "    - print_every: (Optional) Print training accuracy every print_every iterations.\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for.\n",
    "    - stop: (Optional) If True, stops after 100 iterations.\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Move the model parameters to CPU / GPU\n",
    "    model = model.to(device=device)\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for t, train_batch in enumerate(loader_train):\n",
    "            \n",
    "            # Put model to training mode\n",
    "            model.train()\n",
    "            \n",
    "            # Load x and y\n",
    "            x = train_batch.text.transpose(1, 0)  # reshape to [batch_size, len_seq]\n",
    "            y = train_batch.target.type(torch.LongTensor)\n",
    "   \n",
    "            # Move to device, e.g. CPU\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            # Compute scores and softmax loss\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "            \n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                acc = check_accuracy_naive(val_iter, model, validation=True)\n",
    "                print('Accuracy :', acc)\n",
    "                print()\n",
    "                \n",
    "            if stop and t == 100:\n",
    "                break\n",
    "                \n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 0.7549\n",
      "Checking accuracy on validation set\n",
      "Accuracy : 0.9392755681818182\n",
      "\n",
      "Iteration 10, loss = 0.1781\n",
      "Checking accuracy on validation set\n",
      "Accuracy : 0.93359375\n",
      "\n",
      "Iteration 20, loss = 0.2837\n",
      "Checking accuracy on validation set\n",
      "Accuracy : 0.9421164772727273\n",
      "\n",
      "Iteration 30, loss = 0.2444\n",
      "Checking accuracy on validation set\n",
      "Accuracy : 0.9240056818181818\n",
      "\n",
      "Iteration 40, loss = 0.2131\n",
      "Checking accuracy on validation set\n",
      "Accuracy : 0.94140625\n",
      "\n",
      "Iteration 50, loss = 0.1615\n",
      "Checking accuracy on validation set\n",
      "Accuracy : 0.9378551136363636\n",
      "\n",
      "Iteration 60, loss = 0.2342\n",
      "Checking accuracy on validation set\n",
      "Accuracy : 0.9364346590909091\n",
      "\n",
      "Iteration 70, loss = 0.3271\n",
      "Checking accuracy on validation set\n",
      "Accuracy : 0.9364346590909091\n",
      "\n",
      "Iteration 80, loss = 0.2312\n",
      "Checking accuracy on validation set\n",
      "Accuracy : 0.9431818181818182\n",
      "\n",
      "Iteration 90, loss = 0.2182\n",
      "Checking accuracy on validation set\n",
      "Accuracy : 0.9339488636363636\n",
      "\n",
      "Iteration 100, loss = 0.2157\n",
      "Checking accuracy on validation set\n",
      "Accuracy : 0.9435369318181818\n",
      "\n",
      "29.782217264175415\n"
     ]
    }
   ],
   "source": [
    "# Learning rate\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# Model\n",
    "model = NaiveModel(embedding_matrix=word_embeddings, \n",
    "                   hidden_dim=64, \n",
    "                   output_size=2)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=learning_rate, \n",
    "                       betas=(0.9, 0.999),  # recommended values\n",
    "                       eps=1e-08)  # recommended value\n",
    "\n",
    "# Train\n",
    "train_naive(model, optimizer, train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "897 seconds per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zelros_env",
   "language": "python",
   "name": "zelros_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
